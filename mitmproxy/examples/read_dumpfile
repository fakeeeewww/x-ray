#!/usr/bin/env python
#
# Simple script showing how to read a mitmproxy dump file
#

# dependent git repo: https://github.com/mitmproxy/mitmproxy
# doc: 
# manual about mitmproxy: 

from mitmproxy import flow
import pprint
import sys
import re
from harparser import HAR
from datetime import datetime
import json

class _HARLog(HAR.log):
    # The attributes need to be registered here for them to actually be
    # available later via self. This is due to HAREncodable linking __getattr__
    # to __getitem__. Anything that is set only in __init__ will just be added
    # as key/value pair to self.__classes__.
    __page_list__ = []
    __page_count__ = 0
    __page_ref__ = {}

    def __init__(self, page_list):
        self.__page_list__ = page_list
        self.__page_count__ = 0
        self.__page_ref__ = {}

        HAR.log.__init__(self, {"version": "1.2",
                                "creator": {"name": "MITMPROXY HARExtractor",
                                            "version": "0.1",
                                            "comment": ""},
                                "pages": [],
                                "entries": []})

    def reset(self):
        self.__init__(self.__page_list__)

    def add(self, obj):
        if isinstance(obj, HAR.pages):
            self['pages'].append(obj)
        if isinstance(obj, HAR.entries):
            self['entries'].append(obj)
            # print "I am here"

    def create_page_id(self):
        self.__page_count__ += 1
        return "autopage_%s" % str(self.__page_count__)

    def set_page_ref(self, page, ref):
        self.__page_ref__[page] = ref

    def get_page_ref(self, page):
        return self.__page_ref__.get(page, None)

    def get_page_list(self):
        return self.__page_list__

def format_cookies(obj):
    if obj:
        return [{"name": k.strip(), "value": v[0]} for k, v in obj.items()]
    return ""


def format_headers(obj):
    if obj:
        return [{"name": k, "value": v} for k, v in obj.fields]
    return ""

def findHosts (freader):

    # countHost = set()
    try:
        for f in freader.stream():

            request = f.request

            # host = request.host 

            request_query_string = [{"name": k, "value": v}
                for k, v in request.query or {}]
            
            method = request.method
            url = request.url

            bodysize = len(request.content)


            #header = f.request.headers
            header = format_headers(f.request.headers)
            cookies = format_cookies(f.request.cookies)
            print "Header of the request "
            print header
            # print "Header of the request " + header
            print "Content of the request "
            print cookies
            if host not in countHost:
                print "Host " + host
                countHost.add(host)
            #pp.pprint(f.get_state())
            #print("")
            
        # print "found " + str(len(countHost)) + " hosts"
    except flow.FlowReadError as v:
        print "Flow file corrupted. Stopped loading."



def findName (flow):

    # print(flow)

    if flow.get_state().match('junvivek'):
        print("Flow matches filter: Zhao" )
        print(flow)
    # else:
    #     print(flow.get_state())



# def findEmail ():


# def findLocation ():


# def findBehaviour ():


def start(context, argv):

    context.HARLog = _HARLog(['https://github.com'])

    with open('/home/junhao/workspace/mitmproxy/data/pinterest00103', "rb") as logfile:
        freader = flow.FlowReader(logfile)
        pp = pprint.PrettyPrinter(indent=4)

    ## find the hosts accessing the data during the session



    ## find the data accessed by the hosts during the session

    #findDataHosts(freader)

        try:
            for f in freader.stream():
            
                request_query_string = {}
                if f.request.query is not None:
                    request_query_string = [{"name": k, "value": v}
                        for k, v in f.request.query]

                request_http_version = f.request.http_version
                # Cookies are shaped as tuples by MITMProxy.
                request_cookies = [{"name": k.strip(), "value": v[0]}
                    for k, v in f.request.cookies.items()]
            
                request_headers = {}
                if f.request.headers:
                    for k, v in f.request.headers.fields:
                        request_headers["name"] = k
                        request_headers["value"] = v

                request_headers_size = len(str(f.request.headers))
                request_body_size = len(f.request.content)

            

                response_http_version = f.response.http_version
                # Cookies are shaped as tuples by MITMProxy.
                response_cookies = [{"name": k.strip(), "value": v[0]}
                    for k, v in f.response.cookies.items()]

                response_headers = {}
                if f.response.headers:
                    for k, v in f.response.headers.fields:
                        response_headers["name"] = k
                        response_headers["value"] = v

                response_headers_size = len(str(f.response.headers))
                response_body_size = len(f.response.content)
                response_body_decoded_size = len(f.response.get_decoded_content())
                response_body_compression = response_body_decoded_size - response_body_size
                response_mime_type = f.response.headers.get('Content-Type', '')
                response_redirect_url = f.response.headers.get('Location', '') 

                ssl_time = -.001
                connect_time = -.001

                context.seen_server = set()

                if f.server_conn not in context.seen_server:
                    # Calculate the connect_time for this server_conn. Afterwards add it to
                    # seen list, in order to avoid the connect_time being present in entries
                    # that use an existing connection.
                    connect_time = f.server_conn.timestamp_tcp_setup - \
                        f.server_conn.timestamp_start
                    context.seen_server.add(f.server_conn)

                if f.server_conn.timestamp_ssl_setup is not None:
                    # Get the ssl_time for this server_conn as the difference between
                    # the start of the successful tcp setup and the successful ssl
                    # setup. If  no ssl setup has been made it is left as -1 since it
                    # doesn't apply to this connection.
                    ssl_time = f.server_conn.timestamp_ssl_setup - \
                        f.server_conn.timestamp_tcp_setup  

                timings_raw = {
                    'send': f.request.timestamp_end - f.request.timestamp_start,
                    'wait': f.response.timestamp_start - f.request.timestamp_end,
                    'receive': f.response.timestamp_end - f.response.timestamp_start,
                    'connect': connect_time,
                    'ssl': ssl_time
                }

                # HAR timings are integers in ms, so we have to re-encode the raw timings to
                # that format.
                timings = dict([(key, int(1000 * value))
                    for key, value in timings_raw.iteritems()])

                # The full_time is the sum of all timings. Timings set to -1 will be ignored
                # as per spec.
                full_time = 0
                for item in timings.values():
                    if item > -1:
                        full_time += item

                started_date_time = datetime.fromtimestamp(f.request.timestamp_start).isoformat()         


                # for k, v in f.request.query:
                #     if "jun" in v:
                #         print k 

            # find referrer

                entry = HAR.entries(
                    {
                    "startedDateTime": started_date_time,
                    "time": full_time,
                        "request": {
                            "method": f.request.method,
                            "url": f.request.url,
                            "httpVersion": request_http_version,
                            "cookies": request_cookies,
                            # headers are key value pairs 
                            "headers": format_headers(f.request.headers),
                            "queryString": request_query_string,
                            "headersSize": request_headers_size,
                            "bodySize": request_body_size,
                        },
                        "response": {
                            "status": f.response.status_code,
                            "statusText": f.response.msg,
                            "httpVersion": response_http_version,
                            "cookies": response_cookies,
                            "headers": format_headers(f.response.headers),
                            "content": {
                                "size": response_body_size,
                                "compression": response_body_compression,
                                "mimeType": response_mime_type},
                            "redirectURL": response_redirect_url,
                            "headersSize": response_headers_size,
                            "bodySize": response_body_size,
                        },
                        "cache": {},
                        "timings": timings,
                    })

                context.HARLog.add(entry)
                # print "add an entry"



        except flow.FlowReadError as v:
            print "Flow file corrupted. Stopped loading."


        json_dump = context.HARLog.json()    
        # context.log(pprint.pformat(json.loads(json_dump)))

        file('pin-log-dump.json', "w").write(json_dump)
        context.log("HAR log finished with %s bytes " % (len(json_dump)))

    # findHosts(freader)
    # findName(freader)


 
